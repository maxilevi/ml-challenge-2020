{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import helpers\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import collections\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False\n",
    "FACTORS = 100\n",
    "EPOCHS = 20\n",
    "SEARCH_WEIGHT = 0.15\n",
    "ITEMS_PER_SEARCH = 5\n",
    "ITEM_CANDIDATES_PER_USER = 30\n",
    "KAGGLE = False\n",
    "MODEL_TYPE = 'rankfm'#rankfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = helpers.load_items_df()\n",
    "items_dict = helpers.load_items()\n",
    "domain_item_dict = helpers.load_domain_item_dict(items_dict)\n",
    "all_items = list(items_dict.keys())\n",
    "\n",
    "interactions_train = helpers.load_interactions_df()\n",
    "if TEST:\n",
    "    interactions_test = helpers.load_interactions_test_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_times = collections.Counter(interactions_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item_features = pd.get_dummies(items[['item_id', 'domain_id', 'price', 'condition']], columns=['domain_id', 'condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA(512)\n",
    "#x = pca.fit_transform(item_features)\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x.to_csv('./data/item_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_item_features(items, interactions):\n",
    "    viewed_times = collections.Counter(interactions[interactions['event_type'] == 'view'].item_id)\n",
    "    items_df = items[['item_id', 'domain_id', 'price', 'condition']].copy()\n",
    "    #items_df = pd.get_dummies(items_df, columns=['domain_id', 'condition'])\n",
    "    '''\n",
    "    domains = items.domain_id.unique() \n",
    "    m = int(math.log2(len(domains)) + 1)\n",
    "    columns = {f'domain_bit_{i}': [] for i in range(m)}\n",
    "    indexed_domains = {domains[i]: i for i in range(len(domains))}\n",
    "\n",
    "    def domain_apply(x):\n",
    "        arr = helpers.bin_array(indexed_domains[x], m)\n",
    "        for j in range(m):\n",
    "            columns[f'domain_bit_{j}'].append(arr[j])\n",
    "\n",
    "    items_df['domain_id'].apply(domain_apply)\n",
    "'''\n",
    "    #for k in columns.keys():\n",
    "    #    items_df[k] = columns[k]\n",
    "    #le = LabelEncoder()\n",
    "    \n",
    "    items_df['condition'] = items_df['condition'].apply(lambda x: 1 if x == 'new' else 0)\n",
    "    items_df['sold_times'] = items_df['item_id'].apply(lambda x: sold_times[x])\n",
    "    items_df['viewed_times'] = items_df['item_id'].apply(lambda x: viewed_times[x])\n",
    "    #items_df['domain_id'] = le.fit_transform(items_df['domain_id'])\n",
    "    #items_df = items_df.drop(columns=['domain_id'])\n",
    "    #items_df['item_id'] = items_df['item_id'].astype(int)\n",
    "    items_df['price'] = items_df['price'].fillna(0)\n",
    "    #scaler = MinMaxScaler()\n",
    "    #transformed_price = scaler.fit_transform(items_df['price'].values.reshape(-1, 1)).flatten()\n",
    "    #items_df['price'] = pd.Series(transformed_price)\n",
    "    return items_df\n",
    "\n",
    "def encode_user_features(users, interactions):\n",
    "    event_dict = dict(list(interactions.groupby('user_id')))\n",
    "    data = {'user_id': [], 'items_viewed': [], 'searches_done': [], 'categories_viewed': []}\n",
    "    for u in users:\n",
    "        events = event_dict[u].values.tolist()\n",
    "        items_viewed = 0\n",
    "        categories_viewed = set()\n",
    "        searches_done = 0\n",
    "        for event in events:\n",
    "            user_id, info, event_type, timestamp, target = event\n",
    "            if np.isnan(info): \n",
    "                continue\n",
    "            \n",
    "            if event_type == 'search':\n",
    "                searches_done += 1\n",
    "            else:\n",
    "                items_viewed += 1\n",
    "            \n",
    "            categories_viewed.add(items_dict[info]['domain_id'])\n",
    "            \n",
    "        data['user_id'].append(u)\n",
    "        data['items_viewed'].append(items_viewed)\n",
    "        data['searches_done'].append(searches_done)\n",
    "        data['categories_viewed'].append(len(categories_viewed))\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_interactions(df):\n",
    "    new_df = df[pd.notnull(df['item_id'])].copy()\n",
    "    new_df['user_id'] = new_df['user_id'].astype(float).astype(int)\n",
    "    new_df['item_id'] = new_df['item_id'].astype(float).astype(int)\n",
    "    sample_weights = np.array([(3 if x != 'search' else SEARCH_WEIGHT) for x in new_df['event_type']])\n",
    "    return new_df[['user_id', 'item_id']], sample_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_candidate_pairs(users, valid_item_ids):\n",
    "    users_column = []\n",
    "    items_column = []\n",
    "    user_lengths = []\n",
    "    i = 0\n",
    "    for u in users:\n",
    "        candidates = [x for x in get_candidates(u) if x in valid_item_ids]\n",
    "        items_column += candidates\n",
    "        users_column += [u] * len(candidates)\n",
    "        user_lengths.append((u, len(candidates)))\n",
    "        if i % 100000 == 0:\n",
    "            print(f\"Progress {i}/{len(users)}\")\n",
    "        i += 1\n",
    "    pairs = pd.DataFrame({'user_id': users_column, 'item_id': items_column})\n",
    "    return pairs, users_column, items_column, user_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_recommendations(recommendations_pairs, items_column, user_lengths):\n",
    "    offset = 0\n",
    "    recommendations = {}\n",
    "    for user, user_len in user_lengths:\n",
    "        user_recs = recommendations_pairs[offset:offset+user_len]\n",
    "        ranked_recs = np.argsort(user_recs)[::-1]\n",
    "        top_10 = [x for x in ranked_recs if not np.isnan(user_recs[x])][:10]\n",
    "        recommendations[user] = [items_column[x + offset] for x in top_10]\n",
    "        offset += user_len\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains_from_items(items):\n",
    "    return set(items_dict[int(item)]['domain_id'] for item in items)\n",
    "\n",
    "def get_candidates(user):\n",
    "    items_interacted = event_dict[user] if user in event_dict else set()\n",
    "    #k = ITEM_CANDIDATES_PER_USER - len(items_interacted)\n",
    "    #if k > 0:\n",
    "    domains = get_domains_from_items(items_interacted) if items_interacted else top_domains[:10]\n",
    "    items_for_domains = [domain_top_items[d] for d in domains]\n",
    "    item_universe = sum(items_for_domains, [])\n",
    "    #if not item_universe:\n",
    "    #    item_universe = all_items\n",
    "    #extra_items = random.choices(item_universe, k=k)\n",
    "        \n",
    "    for item in item_universe:\n",
    "        items_interacted.add(item)\n",
    "            \n",
    "    return list(items_interacted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_interactions(i1, i2):\n",
    "    i1c = i1.copy()\n",
    "    i2c = i2.copy()\n",
    "    i2c['user_id'] += i1c.shape[0]\n",
    "    return i1c.append(i2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.9 s, sys: 5.03 s, total: 42.9 s\n",
      "Wall time: 43.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "users = None\n",
    "interactions = None\n",
    "sample_weights = None\n",
    "user_features = None\n",
    "\n",
    "if TEST:\n",
    "    interactions = combine_interactions(interactions_train, interactions_test)\n",
    "    validation_users = interactions_test.user_id.unique() + interactions_train.shape[0]\n",
    "    all_users = np.concatenate([interactions_train.user_id.unique(), validation_users])\n",
    "else:\n",
    "    interactions = interactions_train\n",
    "    validation_users = interactions_train.user_id.unique()\n",
    "    all_users = validation_users\n",
    "\n",
    "user_target_dict = None\n",
    "#interactions = interactions[interactions['event_type'] != 'search']\n",
    "\n",
    "## Calculate user features\n",
    "#interactions_users = set(interactions.user_id.dropna().unique())\n",
    "#%time user_features = encode_user_features(interactions_users, interactions)\n",
    "\n",
    "## Calculate item features\n",
    "#interactions_items = set(interactions.item_id.dropna().unique())\n",
    "#items_cp = items.copy()\n",
    "#items_cp.set_index('item_id', inplace=True, drop=False)\n",
    "#%time item_features = encode_item_features(items_cp.loc[interactions_items], interactions)\n",
    "#item_features = item_features.reset_index(drop=True)\n",
    "\n",
    "## Calculate auxiliary data\n",
    "interactions, sample_weights = encode_interactions(interactions)\n",
    "domain_top_items = helpers.load_top_items(interactions_train, domain_item_dict)\n",
    "top_domains = helpers.load_top_domains(interactions_train, domain_top_items)\n",
    "event_dict = interactions.groupby('user_id')['item_id'].unique().apply(set).to_dict()\n",
    "valid_item_ids = set(interactions['item_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    !pip install rankfm\n",
    "    !pip install lightfM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if MODEL_TYPE == 'lightfm':\n",
    "    from lightfm.data import Dataset\n",
    "    from lightfm import LightFM\n",
    "    from lightfm.data import Dataset\n",
    "    from scipy.sparse import coo_matrix\n",
    "    import scipy\n",
    "\n",
    "    item_feature_values = set()\n",
    "    for column in item_features.columns:\n",
    "        if column == 'item_id': continue\n",
    "        item_feature_values |= set(item_features[column].unique())\n",
    "\n",
    "    user_feature_values = set()\n",
    "    for column in user_features.columns:\n",
    "        if column == 'user_id': continue\n",
    "        user_feature_values |= set(user_features[column].unique())\n",
    "\n",
    "    item_ids = interactions['item_id'].unique()\n",
    "    user_ids = all_users\n",
    "\n",
    "    dataset = Dataset()\n",
    "    dataset.fit(\n",
    "        user_ids,\n",
    "        item_ids,\n",
    "        item_features=item_feature_values,\n",
    "        user_features=user_feature_values\n",
    "    )\n",
    "\n",
    "    train_interactions, train_weights = dataset.build_interactions(\n",
    "        ((x[1], x[2], sample_weights[i]) for i, x in enumerate(interactions.itertuples())),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 29.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if MODEL_TYPE == 'lightfm':\n",
    "    lightfm_item_features = dataset.build_item_features(\n",
    "        item_features.apply(lambda x: (x['item_id'], [x[y] for y in x.index[1:] if y != 'item_id']), axis=1).values,\n",
    "    )\n",
    "    lightfm_user_features = dataset.build_user_features(\n",
    "        user_features.apply(lambda x: (x['user_id'], [x[y] for y in x.index[1:] if y != 'user_id']), axis=1).values,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 30.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ITEM_ALPHA = 1e-6\n",
    "if MODEL_TYPE == 'lightfm':\n",
    "    \n",
    "    print(f'Training {train_interactions.shape[0]} samples for {EPOCHS} epochs')\n",
    "    model = LightFM(\n",
    "        no_components=25,\n",
    "        loss='warp',\n",
    "        item_alpha=ITEM_ALPHA\n",
    "    )\n",
    "    ##\n",
    "    ## Try no normalization\n",
    "    ## Try changing the get_candidates() method\n",
    "    ## Do a submit\n",
    "    ## Try using searches to find the domains\n",
    "    ##\n",
    "    model.fit(\n",
    "        train_interactions,\n",
    "        epochs=20,\n",
    "        #sample_weight=train_weights,\n",
    "        #item_features=lightfm_item_features,\n",
    "        #user_features=lightfm_user_features,\n",
    "        num_threads=4,\n",
    "        verbose=True\n",
    "    )\n",
    "    print(\"Building recommendation pairs...\")\n",
    "    pairs, users_column, items_column, user_lengths = build_candidate_pairs(validation_users, valid_item_ids)\n",
    "\n",
    "    user_id_map, user_feature_map, item_id_map, item_feature_map = dataset.mapping()\n",
    "    print(f\"Generating recommendation pairs\")\n",
    "    recommendations_pairs = model.predict(\n",
    "        np.array([user_id_map[x] for x in users_column]),#user_le.transform(users_column),\n",
    "        np.array([item_id_map[x] for x in items_column]),#item_le.transform(items_column),\n",
    "        #item_features=lightfm_item_features,\n",
    "        #user_features=lightfm_user_features,\n",
    "        num_threads=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RankFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 11999164 interactions...\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 16 µs\n",
      "\n",
      "training epoch: 0\n",
      "log likelihood: -6672177.0\n",
      "\n",
      "training epoch: 1\n",
      "log likelihood: -5344543.5\n",
      "\n",
      "training epoch: 2\n",
      "log likelihood: -4493351.5\n",
      "\n",
      "training epoch: 3\n",
      "log likelihood: -3956618.25\n",
      "\n",
      "training epoch: 4\n",
      "log likelihood: -3605009.25\n",
      "\n",
      "training epoch: 5\n",
      "log likelihood: -3357822.75\n",
      "\n",
      "training epoch: 6\n",
      "log likelihood: -3177033.0\n",
      "\n",
      "training epoch: 7\n",
      "log likelihood: -3038952.75\n",
      "\n",
      "training epoch: 8\n",
      "log likelihood: -2930900.0\n",
      "\n",
      "training epoch: 9\n",
      "log likelihood: -2844558.5\n",
      "\n",
      "training epoch: 10\n",
      "log likelihood: -2773588.75\n",
      "\n",
      "training epoch: 11\n",
      "log likelihood: -2713113.25\n",
      "\n",
      "training epoch: 12\n",
      "log likelihood: -2663295.0\n",
      "\n",
      "training epoch: 13\n",
      "log likelihood: -2619548.75\n",
      "\n",
      "training epoch: 14\n",
      "log likelihood: -2580772.25\n",
      "\n",
      "training epoch: 15\n",
      "log likelihood: -2546611.0\n",
      "\n",
      "training epoch: 16\n",
      "log likelihood: -2516111.0\n",
      "\n",
      "training epoch: 17\n",
      "log likelihood: -2489221.0\n",
      "\n",
      "training epoch: 18\n",
      "log likelihood: -2464075.0\n",
      "\n",
      "training epoch: 19\n",
      "log likelihood: -2441660.0\n",
      "Generating candidate pairs\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.3 µs\n",
      "Progress 0/413163\n",
      "Progress 100000/413163\n",
      "Progress 200000/413163\n",
      "Progress 300000/413163\n",
      "Progress 400000/413163\n",
      "Generating recommnedation pairs\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 109 µs\n",
      "CPU times: user 31min 2s, sys: 1min 27s, total: 32min 29s\n",
      "Wall time: 35min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if MODEL_TYPE == 'rankfm':\n",
    "    from rankfm.rankfm import RankFM\n",
    "    \n",
    "    model = RankFM(factors=FACTORS, loss='warp', max_samples=20, alpha=0.01, sigma=0.1, learning_rate=0.10, learning_schedule='invscaling')\n",
    "    \n",
    "    print(f\"Fitting {interactions.shape[0]} interactions...\")\n",
    "    \n",
    "    %time\n",
    "    model.fit(\n",
    "        interactions,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=True,\n",
    "        sample_weight=sample_weights,\n",
    "        #item_features=item_features,\n",
    "        #user_features=user_features\n",
    "    )\n",
    "    \n",
    "    print(f\"Generating candidate pairs\")\n",
    "    \n",
    "    %time\n",
    "    pairs, users_column, items_column, user_lengths = build_candidate_pairs(validation_users, valid_item_ids)\n",
    "    \n",
    "    print(f\"Generating recommnedation pairs\")\n",
    "    \n",
    "    %time\n",
    "    recommendations_pairs = model.predict(pairs, cold_start='nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create item users pairs to feed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 3s, sys: 18.9 s, total: 4min 22s\n",
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "recommendations = build_recommendations(recommendations_pairs, items_column, user_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 entries were not filled. Extending the items...\n"
     ]
    }
   ],
   "source": [
    "def fill(recommendations):\n",
    "    for k in recommendations.keys():\n",
    "        if len(recommendations[k]) == 0:\n",
    "            recommendations[k] = random.choices(all_items, k=10)\n",
    "        elif len(recommendations[k]) < 10:\n",
    "            category = items_dict[recommendations[k][0]]['domain_id']\n",
    "            recommendations[k] += random.choices(domain_item_dict[category], k=(10 - len(recommendations[k])))\n",
    "\n",
    "# Assert required sizes\n",
    "            \n",
    "assert len(recommendations) == len(validation_users)\n",
    "unfilled = len([True for k in recommendations.keys() if len(recommendations[k]) != 10])\n",
    "if unfilled > 0:\n",
    "    print(f\"{unfilled} entries were not filled. Extending the items...\")\n",
    "    fill(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring (if training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TEST and not user_target_dict:\n",
    "    user_target_dict = interactions_train.groupby('user_id')['target'].unique().apply(lambda x: x).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _relevance(items_dict, item, target):\n",
    "    if item == target:\n",
    "        return 15\n",
    "    if items_dict[item]['domain_id'] == items_dict[target]['domain_id']:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def _get_perfect_dcg():\n",
    "    perfect = [15, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "    return sum(perfect[i] / np.log2(i + 2) for i in range(len(perfect))) / len(perfect)\n",
    "\n",
    "def _dcg(items_dict, recommendations, target):\n",
    "    \n",
    "    dcg = sum(_relevance(items_dict, recommendations[i], target) / np.log2(i + 2) for i in range(len(recommendations)))\n",
    "    return dcg / len(recommendations)\n",
    "\n",
    "def ndcg_score(items_dict, recommendations, user_targets_dict):\n",
    "    sum_ndcg = 0\n",
    "    sum_perfect = 0\n",
    "    for x in recommendations.keys():\n",
    "        sum_ndcg += _dcg(items_dict, [int(w) for w in recommendations[x]], int(user_targets_dict[x]))\n",
    "        sum_perfect += _get_perfect_dcg()\n",
    "\n",
    "    return sum_ndcg / sum_perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25357310893765944\n"
     ]
    }
   ],
   "source": [
    "if not TEST:\n",
    "    print(ndcg_score(items_dict, recommendations, user_target_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1276894,\n",
       " 1114176,\n",
       " 1966885,\n",
       " 1338314,\n",
       " 1313192,\n",
       " 1587422,\n",
       " 725371,\n",
       " 928548,\n",
       " 1447583,\n",
       " 1154325]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.25357310893765944\n",
    "#0.25013763660310895\n",
    "#train with search w=0.1 factors=250 item_features=no -> 0.243\n",
    "#train with search w=0.2 factors=150 item_features=no -> 0.24443886619434535\n",
    "#train with search w=0.5 factors=250 item_features=no -> 0.23223811466616529\n",
    "#train with search w=1.0 factors=250 item_features=no -> 0.19637033782036525\n",
    "#train with search w=1.0 factors=100 item_features=no -> 0.197764356297992\n",
    "#train with search w=0.15 factors=100 item_features=no ->0.24546062804586635\n",
    "\n",
    "# lightfm n=50 no search -> 0.2481555866759734\n",
    "# lightfm n=25 -> 0.24950549809750702\n",
    "# lightfm n=25 w integer item features w item alpha -> 0.22521187808442616\n",
    "# lightfm n=25 -> 0.25071840818293245\n",
    "# lightfm n=25 w item alpha -> 0.2508261003278925\n",
    "\n",
    "#lightfm n=25 w search w weights-> 0.2191484993506384\n",
    "\n",
    "# only domain ~0.07660177371164308\n",
    "# only target ~0.16885885433\n",
    "# lightfm only domain with item_f = 0.08535225261531444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating submit (if testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    submit = pd.DataFrame(recommendations)\n",
    "    print(f'Submit shape is {submit.shape}')\n",
    "    assert submit.shape == (10, 177070)\n",
    "    submit.transpose().to_csv(f'submit_f={FACTORS}_e={EPOCHS}.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding domains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
